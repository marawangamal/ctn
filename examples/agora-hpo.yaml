# Hyperparameter optimization
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:a100l:4"
    - "#SBATCH --mem=128G"
    - "#SBATCH --cpus-per-task=12"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/mtl-dev/.venv/bin/activate"

group:
  name: "main"
  type: parallel
  jobs:
    - group:
        name: "stp::hpo"
        type: sequential
        jobs:
          - group:
              type: sweep
              preamble: glong
              sweep:
                lr: [1e-6, 1e-5, 1e-4, 5e-4, 1e-3]
                scheduler: ["cosine"]
              sweep_template:  "HF_HOME=$SCRATCH/huggingface python train_smol.py --tags hpo --disable_evals --limit_train_batches 10000 --limit_val_batches 500 --val_check_interval 2500 --model_head stp --lr {lr} --scheduler {scheduler}"


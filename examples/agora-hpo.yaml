# Hyperparameter optimization
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:a100l:1"
    - "#SBATCH --mem=128G"
    - "#SBATCH --cpus-per-task=12"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/mtl-dev/.venv/bin/activate"

group:
  name: "main"
  type: parallel
  jobs:
    - group:
        name: "stp::hpo"
        type: sequential
        jobs:
          - group:
              type: sweep
              preamble: glong
              sweep:
                lr: [1e-7, 5e-7, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3]
                scheduler: ["none", "cosine"]
              sweep_template:  "HF_HOME=$SCRATCH/huggingface python train_smol.py --tags hpo --disable_evals --limit_train_batches 5000 --limit_val_batches 100 --val_check_interval 1000 --model_head stp --lr {lr} --scheduler {scheduler}"


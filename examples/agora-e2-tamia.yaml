# Experiment 2
# Fine-tuning on OpenMathInstruct-2 with joint loss
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --gres=gpu:4"
    - "#SBATCH --mem=128G"
    - "#SBATCH --cpus-per-task=8"
    - "module load cuda/12.6 arrow python/3.12 httpproxy"
    - "source /scratch/m/mgamal/mtl/.venv/bin/activate"

group:
  name: "main"
  type: sequential
  jobs:

    - group:
        name: "joint::multihead"
        type: sequential
        jobs:
          - job:
              preamble: glong
              command: "WANDB_CACHE_DIR=$SCRATCH/wandb HF_HOME=$SCRATCH/huggingface python train.py --model meta-llama/Llama-3.2-3B-Instruct --dataset omi:1m --model_head multihead --lr 5e-5 --scheduler cosine --loss_type joint --pretrained --max_len 512 --batch_size 8 --epochs 5 --evals gsm8k_cot --limit_val_batches 50"



# lm_eval --model vllm \
#     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.8,data_parallel_size=4 \
#     --tasks gsm8k_cot \
#     --batch_size auto
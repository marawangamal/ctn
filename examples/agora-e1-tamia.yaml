# Experiment 1
# Evaluating efficacy of multi-token regularization during pretraining and finetuning
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --gres=gpu:4"
    - "#SBATCH --mem=128G"
    - "#SBATCH --cpus-per-task=16"
    - "#SBATCH --time=6:00:00"
    - "module load cuda/12.6 arrow python/3.12 httpproxy"
    - "source /scratch/m/mgamal/mtl/.venv/bin/activate"

  clong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --mem=64G"
    - "#SBATCH --cpus-per-task=64"
    - "module load cuda/12.6 arrow python/3.12 httpproxy"
    - "source /scratch/m/mgamal/mtl/.venv/bin/activate"

group:
  name: "main"
  type: parallel
  jobs:
    - group:
        type: sequential
        jobs:
          - group:
              type: parallel
              jobs:
                - job:
                    # FT STP (NOTE: add --max_steps 5 for fast debugging)
                    name: "ft::llama::omi1m::mtp(h=2,lam=0.1)"
                    preamble: glong
                    command: "WANDB_CACHE_DIR=$SCRATCH/wandb HF_HOME=$SCRATCH/huggingface python train_v2.py --tags tamia --eval_limit 50 --lr 1e-7 --lambda_mhead 0.1 --model_head multihead --horizon 2"
 
                - job:
                    # FT STP (NOTE: add --max_steps 5 for fast debugging)
                    name: "ft::llama::omi1m::mtp(h=3,lam=0.1)"
                    preamble: glong
                    command: "WANDB_CACHE_DIR=$SCRATCH/wandb HF_HOME=$SCRATCH/huggingface python train_v2.py --tags tamia --eval_limit 50 --lr 1e-7 --lambda_mhead 0.1 --model_head multihead --horizon 3"

                - job:
                    # FT STP (NOTE: add --max_steps 5 for fast debugging)
                    name: "ft::llama::omi1m::mtp(h=2,lam=1.0)"
                    preamble: glong
                    command: "WANDB_CACHE_DIR=$SCRATCH/wandb HF_HOME=$SCRATCH/huggingface python train_v2.py --tags tamia --eval_limit 50 --lr 1e-7 --lambda_mhead 1.0 --model_head multihead --horizon 2"

                - job:
                    # FT STP (NOTE: add --max_steps 5 for fast debugging)
                    name: "ft::llama::omi1m::mtp(h=3,lam=1.0)"
                    preamble: glong
                    command: "WANDB_CACHE_DIR=$SCRATCH/wandb HF_HOME=$SCRATCH/huggingface python train_v2.py --tags tamia --eval_limit 50 --lr 1e-7 --lambda_mhead 1.0 --model_head multihead --horizon 3"

 
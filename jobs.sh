WANDB_CACHE_DIR=$SCRATCH/wandb HF_HOME=$SCRATCH/huggingface python train_v2.py --tags tamia --eval_limit 100 --lr 1e-7 --lambda_mhead 0.0 --model_head stp --horizon 1 --model meta-llama/Llama-3.2-1B --tokenizer meta-llama/Llama-3.2-1B-instruct --dataset gsm8k --eval_every 230 --max_epochs 5
WANDB_CACHE_DIR=$SCRATCH/wandb HF_HOME=$SCRATCH/huggingface python train_v2.py --tags tamia --eval_limit 100 --lr 1e-7 --lambda_mhead 0.1 --model_head stp --horizon 1 --model meta-llama/Llama-3.2-1B --tokenizer meta-llama/Llama-3.2-1B-instruct --dataset gsm8k --eval_every 230 --max_epochs 5
WANDB_CACHE_DIR=$SCRATCH/wandb HF_HOME=$SCRATCH/huggingface python train_v2.py --tags tamia --eval_limit 100 --lr 1e-7 --lambda_mhead 0.1 --model_head multihead --horizon 2 --model meta-llama/Llama-3.2-1B --tokenizer meta-llama/Llama-3.2-1B-instruct --dataset gsm8k --eval_every 230 --max_epochs 5

WANDB_CACHE_DIR=$SCRATCH/wandb HF_HOME=$SCRATCH/huggingface python train_v2.py --tags tamia --eval_limit 100 --lr 1e-7 --lambda_mhead 0.0 --model_head stp --horizon 1 --model google/gemma-2b --tokenizer google/gemma-2b-it --dataset gsm8k --eval_every 230 --max_epochs 5
WANDB_CACHE_DIR=$SCRATCH/wandb HF_HOME=$SCRATCH/huggingface python train_v2.py --tags tamia --eval_limit 100 --lr 1e-7 --lambda_mhead 0.1 --model_head stp --horizon 1 --model google/gemma-2b --tokenizer google/gemma-2b-it --dataset gsm8k --eval_every 230 --max_epochs 5
WANDB_CACHE_DIR=$SCRATCH/wandb HF_HOME=$SCRATCH/huggingface python train_v2.py --tags tamia --eval_limit 100 --lr 1e-7 --lambda_mhead 0.1 --model_head multihead --horizon 2 --model google/gemma-2b --tokenizer google/gemma-2b-it --dataset gsm8k --eval_every 230 --max_epochs 5
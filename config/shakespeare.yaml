# Experiment 1
# Evaluating efficacy of multi-token regularization during pretraining and finetuning
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:rtx8000:1"
    - "#SBATCH --mem=32G"
    - "#SBATCH --cpus-per-task=8"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/mtl/.venv/bin/activate"

group:
  name: "main"
  type: parallel
  jobs:
    # Shakespeare | MPSProj 
    - group:
        name: "mnist::cp"
        type: sweep
        preamble: glong
        sweep:
          lr: [1e-3, 5e-4, 1e-4]
          rank: [8, 32, 64, 128]
          model: ["cp_decoder"]
          pos_func: ["square", "sigmoid", "exp"]
        sweep_template: "python train.py --rank {rank} --model {model} --pos_func {pos_func} --lr {lr}"



    # Shakespeare | MPSProj 
    - group:
        name: "mnist::moe"
        type: sweep
        preamble: glong
        sweep:
          lr: [1e-3, 5e-4, 1e-4]
          rank: [8, 32, 64, 128]
          model: ["moe_decoder"]
          lm_head_load_balance_lambda: [0.0, 0.1, 0.5, 1.0]
        sweep_template: "python train.py --rank {rank} --model {model}  --lm_head_load_balance_lambda {lm_head_load_balance_lambda}"


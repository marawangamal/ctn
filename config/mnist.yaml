# Experiment 1
# Evaluating efficacy of multi-token regularization during pretraining and finetuning
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:rtx8000:1"
    - "#SBATCH --mem=32G"
    - "#SBATCH --cpus-per-task=8"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/mtl/.venv/bin/activate"

group:
  name: "main"
  type: parallel
  jobs:
    # Shakespeare | MPSProj 
    - group:
        name: "mnist::cp"
        type: sweep
        preamble: glong
        sweep:
          rank: [10, 64]
          model: ["cp", "moe"]
          pos_func: ["square", "sigmoid", "exp"]
        sweep_template: "python scripts/train_mnist.py --rank {rank} --model {model} --pos_func {pos_func}"

